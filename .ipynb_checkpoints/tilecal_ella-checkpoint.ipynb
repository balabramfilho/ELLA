{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ELLA import ELLA\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from scipy.linalg import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def multi_task_train_test_split(Xs,Ys,train_size=0.5):\n",
    "    Xs_train = []\n",
    "    Ys_train = []\n",
    "    Xs_test = []\n",
    "    Ys_test = []\n",
    "    for t in range(len(Xs)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xs[t], np.squeeze(Ys[t]), train_size=train_size)\n",
    "        Xs_train.append(X_train)\n",
    "        Xs_test.append(X_test)\n",
    "        Ys_train.append(y_train)\n",
    "        Ys_test.append(y_test)\n",
    "    return Xs_train, Xs_test, Ys_train, Ys_test\n",
    "\n",
    "def multi_task_KFold(Xs,Ys,n_splits=5):\n",
    "    Xs_train = {}\n",
    "    Ys_train = {}\n",
    "    Xs_test = {}\n",
    "    Ys_test = {}\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for t in range(len(Xs)):\n",
    "        for train_index, test_index in kf.split(Xs[t]):\n",
    "            Xs_train.append(Xs[t][train_index])\n",
    "            Xs_test.append(Xs[t][test_index])\n",
    "            Ys_train.append(Ys[t][train_index])\n",
    "            Ys_test.append(Ys[t][test_index])\n",
    "    return Xs_train, Xs_test, Ys_train, Ys_test\n",
    "\n",
    "def read_data(dataPath,dirName,module_channel,method):\n",
    "    df_samples = pd.read_csv(os.path.join(dataPath,dir_name,dir_name+'_'+module_channel+'.csv'),sep=' ')\n",
    "    df_method = pd.read_csv(os.path.join(dataPath,dir_name,method+'_'+module_channel+'.csv'),sep=' ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBAmu30\n",
      "LBAmu50\n",
      "LBAmu90\n",
      "Average explained variance score 0.9999994663484998\n",
      "RMSE:  [4.7196189206053006e-05, 5.0539496138606485e-06, 4.541935039586966e-06]\n",
      "Average explained variance score 0.9999982881929966\n",
      "RMSE:  [0.00018739304873389065, 9.487332849274088e-10, 7.282944607323084e-10]\n",
      "Average explained variance score 0.999996158762499\n",
      "RMSE:  [0.00041962288438899447, 1.0094752546362694e-09, 7.775121743908014e-10]\n"
     ]
    }
   ],
   "source": [
    "fileDir = ['LBAmu30','LBAmu50','LBAmu90']\n",
    "method = 'outOF'\n",
    "module_channel = 'm_7_c_46'\n",
    "dataPath = 'D:\\\\Scripts\\\\LPS\\\\data\\\\'\n",
    "filter_param = 'amp'\n",
    "n_splits = 10\n",
    "tasks = {0}\n",
    "\n",
    "# get the array with the lowest size\n",
    "lower_size = np.inf\n",
    "for dir_name in fileDir:\n",
    "    df_samples = pd.read_csv(os.path.join(dataPath,dir_name,dir_name+'_'+module_channel+'.csv'),sep=' ')\n",
    "    df_method = pd.read_csv(os.path.join(dataPath,dir_name,method+'_'+module_channel+'.csv'),sep=' ')\n",
    "    if lower_size > df_samples.shape[0]:\n",
    "        lower_size = df_samples.shape[0]\n",
    "lower_size\n",
    "\n",
    "input_samples = np.array([])\n",
    "# read data\n",
    "for dir_name in fileDir:\n",
    "    df_samples = pd.read_csv(os.path.join(dataPath,dir_name,dir_name+'_'+module_channel+'.csv'),sep=' ', nrows=lower_size)\n",
    "    df_method = pd.read_csv(os.path.join(dataPath,dir_name,method+'_'+module_channel+'.csv'),sep=' ', nrows=lower_size)\n",
    "    print(dir_name)\n",
    "    if input_samples.size == 0:\n",
    "        input_samples = np.expand_dims(df_samples.values, axis=0)\n",
    "        target_amp = np.expand_dims(df_method[filter_param].values, axis=0)\n",
    "    else:\n",
    "        input_samples = np.append(input_samples,np.expand_dims(df_samples.values, axis=0),axis=0)\n",
    "        target_amp = np.append(target_amp,np.expand_dims(df_method[filter_param].values, axis=0),axis=0)\n",
    "\n",
    "# split in train and test\n",
    "Xs_train, Xs_test, Ys_train, Ys_test = multi_task_KFold(input_samples,target_amp,n_splits=n_splits)\n",
    "\n",
    "# set up ELLA\n",
    "T = len(fileDir)\n",
    "d = df_samples.shape[1]\n",
    "k = d # same latent as features\n",
    "base_learner = Ridge(random_state=0) # LinearRegression, Ridge, or LogisticRegression\n",
    "\n",
    "# create ELLA\n",
    "model = ELLA(d,k,Ridge,mu=1,lam=10**-5) \n",
    "\n",
    "# train ELLA\n",
    "for x in range(n_splits):\n",
    "    for t in range(T):\n",
    "        model.fit(Xs_train[t], Ys_train[t], t)\n",
    "    \n",
    "    # test ELLA    \n",
    "    print(\"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)]))\n",
    "    RMSE_error = [mean_squared_error(Ys_test[t], model.predict(Xs_test[t], t)) for t in range(T)]\n",
    "    print(\"RMSE: \",RMSE_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = ['LBAmu30','LBAmu50','LBAmu90']\n",
    "taskName = fileDir\n",
    "method = 'outOF'\n",
    "module_channel = 'm_7_c_46'\n",
    "dataPath = 'D:\\\\Scripts\\\\LPS\\\\data\\\\'\n",
    "filter_param = 'amp'\n",
    "n_splits = 10\n",
    "version = '1'\n",
    "input_data = {}\n",
    "target_data = {}\n",
    "\n",
    "# read data\n",
    "lower_size = np.inf\n",
    "for dir_name in fileDir:\n",
    "    df_samples = pd.read_csv(os.path.join(dataPath,dir_name,dir_name+'_'+module_channel+'.csv'),sep=' ')\n",
    "    df_method = pd.read_csv(os.path.join(dataPath,dir_name,method+'_'+module_channel+'.csv'),sep=' ')\n",
    "    input_data[dir_name] = df_samples.values\n",
    "    target_data[dir_name] = df_method.values\n",
    "\n",
    "# set up ELLA\n",
    "T = len(fileDir)\n",
    "d = df_samples.shape[1]\n",
    "k = d # same latent as features\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "kfold_nr=0\n",
    "for train_index, test_index in kf.split(Xs[t]):\n",
    "    model = ELLA(d,k,Ridge,mu=1,lam=10**-5)  # LinearRegression, Ridge, or LogisticRegression\n",
    "\n",
    "    for t in taskName:\n",
    "        Xs_train, Xs_test, Ys_train, Ys_test = train_test_split(input_data[t],target_data[t])\n",
    "        model.fit(Xs_train[t], Ys_train[t], t)\n",
    "    \n",
    "    print(\"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)]))\n",
    "    RMSE_error = [mean_squared_error(Ys_test[t], model.predict(Xs_test[t], t)) for t in range(T)]\n",
    "    print(\"RMSE: \",RMSE_error)\n",
    "    \n",
    "    print('model_{}_t_{}_kfold_{}.pkl'.format(dt,task,kfold_nr))\n",
    "        \n",
    "    kfold_nr+=1\n",
    "# create ELLA\n",
    "model = ELLA(d,k,Ridge,mu=1,lam=10**-5) \n",
    "\n",
    "# train ELLA\n",
    "for x in range(n_splits):\n",
    "    for t in range(T):\n",
    "        model.fit(Xs_train[t], Ys_train[t], t)\n",
    "    \n",
    "    # test ELLA    \n",
    "    print(\"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)]))\n",
    "    RMSE_error = [mean_squared_error(Ys_test[t], model.predict(Xs_test[t], t)) for t in range(T)]\n",
    "    print(\"RMSE: \",RMSE_error)\n",
    "    \n",
    "#     dt = datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "#     with open('model_{}_t_{}_kfold_{}.pkl'.format(dt,task,kfold_nr), 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#         pickle.dump([model, obj1, obj2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for x in range(n_splits):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-16 22:13:02'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dt = datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_append_dispatcher() missing 1 required positional argument: 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-32fb0d69fe4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _append_dispatcher() missing 1 required positional argument: 'values'"
     ]
    }
   ],
   "source": [
    "np.append(np.expand_dims(df_samples.values, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ELLA' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-fd5dede08c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ELLA' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_samples = np.expand_dims(df_samples.values, axis=0)\n",
    "input_samples = np.append(input_samples,np.expand_dims(df_samples.values, axis=0),axis=0)\n",
    "input_samples = np.append(input_samples,np.expand_dims(df_samples.values, axis=0),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_test, Ys_train, Ys_test = multi_task_KFold(input_samples,target_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10422, 7)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.38918069, -2.64799956,  0.49014303, ..., -0.80053477,\n",
       "        1.16354503,  7.57075783])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
