{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average explained variance score 0.8304393148382092\n",
      "Average classification accuracy 0.8819999999999999\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def multi_task_train_test_split(Xs,Ys,train_size=0.5):\n",
    "    Xs_train = []\n",
    "    Ys_train = []\n",
    "    Xs_test = []\n",
    "    Ys_test = []\n",
    "    for t in range(len(Xs)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xs[t], np.squeeze(Ys[t]), train_size=train_size)\n",
    "        Xs_train.append(X_train)\n",
    "        Xs_test.append(X_test)\n",
    "        Ys_train.append(y_train)\n",
    "        Ys_test.append(y_test)\n",
    "    return Xs_train, Xs_test, Ys_train, Ys_test\n",
    "\n",
    "from ELLA import ELLA\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from scipy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "T = 20\n",
    "d = 10\n",
    "n = 100\n",
    "k = 5\n",
    "noise_var = .1\n",
    "\n",
    "model = ELLA(d,k,Ridge,mu=1,lam=10**-5)\n",
    "\n",
    "S_true = np.random.randn(k,T)\n",
    "L_true = np.random.randn(d,k)\n",
    "w_true = L_true.dot(S_true)\n",
    "\n",
    "# make sure to add a bias term (it is not done automatically)\n",
    "Xs = [np.hstack((np.random.randn(n,d-1), np.ones((n,1)))) for i in range(T)]\n",
    "# generate the synthetic labels\n",
    "Ys = [Xs[i].dot(w_true[:,i]) + noise_var*np.random.randn(n,) for i in range(T)]\n",
    "# break into train and test sets\n",
    "Xs_train, Xs_test, Ys_train, Ys_test = multi_task_train_test_split(Xs,Ys,train_size=0.5)\n",
    "\n",
    "for t in range(T):\n",
    "    model.fit(Xs_train[t], Ys_train[t], t)\n",
    "print(\"Average explained variance score\", np.mean([model.score(Xs_test[t], Ys_test[t], t) for t in range(T)]))\n",
    "\n",
    "# Try out a classification problem\n",
    "Ys_binarized_train = [Ys_train[i] > 0 for i in range(T)]\n",
    "Ys_binarized_test = [Ys_test[i] > 0 for i in range(T)]\n",
    "\n",
    "model = ELLA(d,k,LogisticRegression,mu=1,lam=10**-5)\n",
    "for t in range(T):\n",
    "    model.fit(Xs_train[t], Ys_binarized_train[t], t)\n",
    "\n",
    "print(\"Average classification accuracy\", np.mean([model.score(Xs_test[t], Ys_binarized_test[t], t) for t in range(T)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100, 10)\n",
      "(20, 100)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(Xs).shape)\n",
    "print(np.array(Ys).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.32435098 -0.28570778 -0.2712983  -0.26299444 -0.29489066 -0.29085745\n",
      "  -0.30831645 -0.2853992  -0.29059767 -0.29244561 -0.28854514 -0.30840084\n",
      "  -0.29665186 -0.32287709 -0.30121575 -0.20919978 -0.18757392 -0.23616294\n",
      "  -0.19422565 -0.18789317]]\n",
      "Average AUC: 0.8597029383274136\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = loadmat('landminedata.mat')\n",
    "\n",
    "Xs_lm = []\n",
    "Ys_lm = []\n",
    "for t in range(data['feature'].shape[1]):\n",
    "    X_t = data['feature'][0,t]\n",
    "    Xs_lm.append(np.hstack((X_t,np.ones((X_t.shape[0],1)))))\n",
    "    Ys_lm.append(data['label'][0,t] == 1.0)\n",
    "\n",
    "d = Xs_lm[0].shape[1]\n",
    "k = 1\n",
    "\n",
    "Xs_lm_train, Xs_lm_test, Ys_lm_train, Ys_lm_test = multi_task_train_test_split(Xs_lm,Ys_lm,train_size=0.5)\n",
    "model = ELLA(d,k,LogisticRegression,{'C':10**0},mu=1,lam=10**-5)\n",
    "for t in range(T):\n",
    "    model.fit(Xs_lm_train[t], Ys_lm_train[t], t)\n",
    "\n",
    "print(model.S)\n",
    "\n",
    "print(\"Average AUC:\", np.mean([roc_auc_score(Ys_lm_test[t],\n",
    "                                             model.predict_logprobs(Xs_lm_test[t], t))\n",
    "                               for t in range(1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xs_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
